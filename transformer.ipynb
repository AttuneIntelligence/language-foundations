{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer\n",
    "\n",
    "In this week's Virtual Lab, you will write your own implementation of ChatGPT! We will build a transformer architecture from scratch, learning as we go why each component is necessary and how each network component contributes to the overall language network's function.\n",
    "\n",
    "The key idea behind the Transformer is that it uses a mechanism called self-attention to weigh the importance of different parts of the input sequence when making predictions about the output sequence. This is in contrast to earlier models that used recurrence or convolution to process sequences, which have limitations when it comes to capturing long-range dependencies.\n",
    "\n",
    "The Transformer consists of a series of blocks, each of which contains a self-attention mechanism followed by a feedforward neural network. The self-attention mechanism computes a weighted sum of the input sequence, with the weights determined by the similarity between each element of the sequence and every other element. This allows the model to attend to different parts of the input sequence depending on the task at hand. The feedforward neural network then applies a non-linear transformation to the weighted sum to produce the output of the block.\n",
    "\n",
    "The Bigram Language Model, which we will use in our Transformer, takes a sequence of integer indices representing characters in a text and learns to predict the next character in the sequence. It does this by applying a series of transformer blocks to the input sequence, and then passing the output through a linear layer to obtain logits, which represent the model's confidence in each possible next character. During training, the model is given the ground-truth next character as a target, and the loss is computed as the cross-entropy between the logits and the target. During generation, the model is given a starting sequence and generates new characters one at a time by sampling from the distribution of logits.\n",
    "\n",
    "This notebook is adapted from Andrej Karpathy's GPT [Zero to Hero](https://youtu.be/kCc8FmEb1nY) example, where he spells out how to build a language model that can generate real text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and formatting data\n",
    "\n",
    "First, let's explore the data we will be training our transformer with. We will be using the `tinyshakespeare` dataset, which is simply a text file containing a vast collection of Shakespeare's writing. We will be training our transformer to generate text in the language of Shakespeare, character by character. Let's explore the data!\n",
    "\n",
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('/home/student/Desktop/classroom/starter/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total length of dataset in characters: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the first 1000 characters...\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "all_chars = ''.join(chars)\n",
    "print(f'Distinct characters: {all_chars}')\n",
    "print(f'Number of unique characters: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data we will be \"learning\", we can see that it is a script of Shakespeare in plain-text format.\n",
    "\n",
    "Now, we need to create our encoding from characters to a numerical vector embedding. Transformers require that the data be provided in vector notation. How will we do this?\n",
    "\n",
    "### Vector Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"Hi! My name is Pria.\"))\n",
    "print(decode(encode(\"Hi! My name is Pria.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is this doing? We are trying to encode our text file into a numerical representation, and in this case we are just taking the character vocabulary we've defined and using it as our direct mapping. For example, we can see that the first letter of our string is `H`. This corresponds to the numerical integer of `20`, according to our encoder. Can we see where this is defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a relatively simple mapping to our pre-defined vocabulary of vectors. Many of the more complex Language Models use much longer vocabularies that contain multiple characters or even words per token, however for the sake of simplicity we will complete this tutorial using character-based encoding with a simple vocabulary. Dive deeper into something like [tiktoken](https://github.com/openai/tiktoken) if you're interested to learn about more complex encodings!\n",
    "\n",
    "Now that we have our encoder defined, we need to turn our Shakespeare data into a vector encoded format...\n",
    "\n",
    "We will create a massive **tensor** of the whole dataset.\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch and are used to represent data and parameters in neural networks. Tensors are similar to NumPy arrays, but they come with additional features specifically designed for deep learning. Some key features of PyTorch tensors include:\n",
    "\n",
    "* GPU support: Tensors can be easily moved to a GPU (Graphics Processing Unit) for accelerated computation. This is especially useful for training and running deep learning models, as GPUs can perform calculations much faster than CPUs (Central Processing Units) in many cases.\n",
    "\n",
    "* Automatic differentiation: PyTorch tensors support automatic differentiation, which is a technique used to compute gradients (derivatives) required for optimizing neural network parameters. This makes training models in PyTorch more efficient and easier to implement.\n",
    "\n",
    "* Dynamic computation graph: PyTorch tensors allow for dynamic computation graphs, meaning the structure of the neural network can change during runtime. This provides more flexibility when designing and experimenting with complex models.\n",
    "\n",
    "You can create tensors in PyTorch with various data types (e.g., float, int) and shapes (e.g., scalar, vector, matrix). Tensors can be created from existing data, like Python lists or NumPy arrays, or initialized with specific values, such as zeros or random numbers. Once created, you can perform mathematical operations, reshape tensors, and manipulate their data to build and train machine learning models. In this case, we are creating one big tensor with our Shakespeare data, and then splitting it into training and testing sets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(train_data[:300])\n",
    "\n",
    "tensor_len = len(train_data) + len(val_data)\n",
    "print(f'Total tensor length: {tensor_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Now, we can see that we have a numerical representation of our data of the same character length of the original text. This is because we are encoding with a 1:1 character:token ratio. Again, we see a much larger ratio in current Large Language Models that can handle larger vocabularies and more complex embeddings. For now, this is great for us to learn!\n",
    "\n",
    "Next, we need to chunk our data. This involves intelligently splitting our input data in such a way that we can pass modular, bite-sized \"chunks\" of data through the network for our Transformer to begin to learn. Chunking is necessary for a few reasons:\n",
    "\n",
    "* **Memory constraints**: Transformers have a self-attention mechanism that computes relationships between all pairs of input elements (e.g., words or tokens). This results in a quadratic increase in memory requirements with respect to sequence length. For long sequences, the memory needed to store intermediate values during training can quickly exceed the available GPU memory. By breaking the input into smaller chunks or blocks, you can fit the model and its intermediate values into the memory, allowing for efficient training.\n",
    "\n",
    "* **Computational efficiency**: When working with long sequences, the computational cost of the self-attention mechanism can be high due to the quadratic complexity. By chunking the input, you can significantly reduce the number of calculations needed, making the training process faster and more efficient.\n",
    "\n",
    "* **Gradient propagation**: For very long sequences, gradient propagation through the layers of the Transformer during backpropagation can become challenging. Gradients can vanish or explode, making it difficult for the model to learn effectively. Chunking the input into smaller blocks helps mitigate this issue by reducing the sequence length and making gradient propagation more stable.\n",
    "\n",
    "* **Parallelization**: Training deep learning models, including Transformers, can be time-consuming. By dividing the input into smaller chunks or blocks, you can take advantage of parallelization, distributing the computation across multiple GPUs or other hardware accelerators. This can speed up training and make the process more efficient.\n",
    "\n",
    "The block size, then, is defined as the maximum number of characters that can be passed to the model in any given training step. We can start to visualize this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this results in a tensor of length 9. Why the extra character? Let's see what this is doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1] # so y is defined as one step up the string...\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this look like if we see just the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size_txt = 13\n",
    "\n",
    "x_txt = text[:block_size_txt]\n",
    "y_txt = text[1:block_size_txt+1] # so y is defined as one step up the string...\n",
    "print(f\"X Text: {x_txt}\")\n",
    "print(f\"Y Text: {y_txt}\\n\")\n",
    "\n",
    "for t in range(block_size_txt):\n",
    "    context = x_txt[:t+1]\n",
    "    target = y_txt[t]\n",
    "    print(f\"when input is [{context}] the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that because we've defined our `Y` variable to be one character ahead of our X data, we are in essence passing the transformer a sliding window of text that has been converted to numbers.\n",
    "\n",
    "With this, we can also define a **batch**. This allows the transformer to process multiple chunks at a time, keeping the GPUs/CPUs that we're working with busy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(999)\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f'when input is [{context.tolist()}] the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this single tensor, we have defined 32 (`8 * 4`) seperate inputs for training our transformer. Let's start modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing the Bigram Language Model\n",
    "\n",
    "A Bigram Language Model is a simple language model that predicts the next word (token) in a sequence based on the current word only. It estimates the probability of a word following another word in a text corpus. In other words, it relies on the conditional probability of a word given its immediate predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        # This stands for batch, time, channel\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a simple bigram language model using PyTorch. The key parts of this code to understand are:\n",
    "\n",
    "1. **forward()**: This function computes the logits for each token in the input sequence idx. If targets are provided, it also calculates the cross-entropy loss between the predicted logits and the target tokens. \n",
    "    * By defining `(B, T, C)` as `Batch` (size of our text chunk), `Time` (which step in that chunk we are at), and `Channel` (our character vocabulary), we are taking every input and passing them as their own tensor to the model.\n",
    "    * We can use these to then define the `logits`, which are our probability distributions for the prediction of the next character in that input tensor\n",
    "    \n",
    "\n",
    "2. **generate()**: This function generates a new sequence of tokens by iteratively sampling from the probability distribution of the next token, given the current context. It takes the initial context idx and the number of new tokens to generate max_new_tokens as input, and returns the generated sequence.\n",
    "\n",
    "When put together, this model can be used to predict the next token (or simply character, in this case) that will come up.\n",
    "\n",
    "Now, we can now train this model through a series of learning steps! We will use the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100):\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not English, and definitely is not Shakespeare...\n",
    "\n",
    "What if we try training it for another `10,000` epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the loss decrease, but this still doesn't seem to make much sense... It is not English, and certainly is not Shakespeare! There's a problem...\n",
    "\n",
    "It predicts the next token based *solely* on the current token. In order to really feel like we are reanimating Shakespeare, we need to explore how these character-predictions can start talking to each other in order to understand context and semantic meaning within text.\n",
    "\n",
    "And we arive to..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Transformer\n",
    "\n",
    "### The Self-Attention Mechanism\n",
    "\n",
    "Self-attention is a key concept inside a Transformer that helps the model understand how important different words in a sentence are in relation to each other. Think of it as the Transformer's way of paying attention to different parts of the input when making predictions.\n",
    "\n",
    "Imagine you're reading a sentence and trying to figure out what it means. You naturally pay more attention to certain words that help you understand the main idea, while other words might not be as important. Self-attention works similarly in a Transformer.\n",
    "\n",
    "When a Transformer processes a sentence, it looks at all the words in the sentence simultaneously. For each word, it calculates a score that represents how important that word is in relation to the word it's trying to predict. These scores are used as weights to create a weighted average of all the words in the sentence. This weighted average is then used as input for the next layer in the Transformer.\n",
    "\n",
    "By using self-attention, the Transformer can understand the relationships between words in a sentence, no matter how far apart they are. This allows the model to capture complex patterns and dependencies in the text, making it great at tasks like language translation, text generation, and more.\n",
    "\n",
    "Let's look at a dummy-example of the math behind this self-attention mechanism..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dummy example of self-attention with matrix multiplication\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is doing the following:\n",
    "1. Creates a matrix `a` of shape (3, 3) with all ones.\n",
    "2. Normalize the rows of matrix `a` by dividing each element by the sum of the corresponding row, making sure the sum of each row is 1. This creates a probability distribution over rows (or sentence elements to be weighed).\n",
    "3. Create a random matrix `b` of shape (3, 2) with elements from 0 to 9.\n",
    "4. Perform matrix multiplication between 'a' and 'b', resulting in matrix 'c' of shape (3, 2).\n",
    "\n",
    "The key concept in this code is the matrix multiplication between matrices 'a' and 'b' (a @ b). In the context of self-attention, matrix 'a' represents the attention weights, where each row contains probabilities that determine how much each element in 'b' contributes to the final output. By performing the matrix multiplication, we obtain a weighted aggregation (matrix 'c'), where each element is a combination of the elements in 'b', with weights determined by the corresponding row in 'a'. \n",
    "\n",
    "**The model can now pay attention to context...**\n",
    "\n",
    "So lets take that principle and apply it to an actual Transformer. Here, I will define all of the code, redefining many of the functions we've already worked through...\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Set hyperparameters like batch size, block size, maximum number of iterations, learning rate, number of embeddings (n_embd), number of heads (n_head), number of layers (n_layer), dropout, and device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # we will just be using CPU\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "torch.manual_seed(999)\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Characters and Mapping\n",
    "\n",
    "Create a list of unique characters in the text and create a mapping from each character to an integer value. This allows the text to be represented as a sequence of integers rather than a sequence of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Splits\n",
    "\n",
    "Split the data into a training set (90% of data) and validation set (10% of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading Function\n",
    "\n",
    "Create a function that generates a small batch of data for inputs x and targets y from either the training or validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Estimation Function\n",
    "\n",
    "Create a function that estimates the loss for the training and validation set by iterating through several batches of data and averaging the loss across batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Head Module\n",
    "\n",
    "Create a class for one head of self-attention in the transformer. This includes linear transformations for the key, query, and value inputs, as well as computing attention scores (\"affinities\"), performing weighted aggregation of the values, and applying dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Create a class for multiple heads of self-attention in the transformer. This includes several heads created from the Head class, a linear transformation layer, and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Class\n",
    "\n",
    "Create a class for a simple linear layer followed by a non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Class\n",
    "\n",
    "Create a class for a transformer block that includes communication followed by computation. This includes a multi-head attention layer, a feed forward layer, and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Language Model Class\n",
    "\n",
    "Create a class for a super simple bigram language model that includes an embedding layer for tokens and positions, several transformer blocks, layer normalization, and a linear head for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Create a PyTorch optimizer using the AdamW algorithm and the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Train the model by iterating through the training data and evaluating the loss at set intervals. The loss is then backpropagated through the model to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Function\n",
    "\n",
    "Generate text from the trained model by passing in a context (a tensor of shape (1,1) initialized to all zeros) and generating new tokens one at a time using the probability distribution output by the model. This process is repeated for a maximum number of tokens to generate a longer sequence of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that even though this isn't great, it is much better! We need to keep in mind that the model we've built it only trained to predict one character at a time, so it still struggles to make realistic English text. It might not understand the English language, but we can see how the context is much better than the raw Bigram model. It look like the Shakespeare text we input!\n",
    "\n",
    "In order to create a true English language model, however, we will need to expand the vocabulary to include full words rather than single characters. This exactly what ChatGPT does, and we will explore more of this in the following lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-foundations",
   "language": "python",
   "name": "ai-foundations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
